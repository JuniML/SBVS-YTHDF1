{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac30f3d-3c62-4acc-8727-f2f36812fc0b",
   "metadata": {},
   "source": [
    "# Run Our trained Scoring function on example molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517ff8e-11fe-4465-aafc-0c466220325e",
   "metadata": {},
   "source": [
    "This Jupyter Notebook will helps users to to run our trained scoring function ANN_PLEC on their own molecules. In this Jupyter Notebook, We have used test set molecules which contain 8 actives and 330 inactives (deepcoy decoys)\n",
    "\n",
    "Additional information can be found in our Nature Protocols paper: Tran-Nguyen, V. K., Junaid, M., Simeon, S. & Ballester, P. J. A practical guide to machine-learning scoring for structure-based virtual screening. Nat. Protoc. (2023)\n",
    "\n",
    "We recommend users to set up the protocol-env environment before running the code in this Jupyter notebook. This can be done using the protocol-env.yml file in our MLSF-protocol github repository: https://github.com/vktrannguyen/MLSF-protocol.\n",
    "\n",
    "For deepcoys generation, please use the github repository https://github.com/fimrie/DeepCoy/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b5e7d-d9bf-4c7f-bc60-b7378508fb9d",
   "metadata": {},
   "source": [
    "## 1. Import all necessary Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809fb19c-804a-454e-9036-f672e00350a3",
   "metadata": {},
   "source": [
    "The following libraries/packages/toolkits need to be installed beforehand: jupyter notebook, pandas, oddt, sklearn, xgboost, rdkit, deepchem, joblib, tqdm, glob, tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164d697-b6bc-4581-a821-c7c1522d2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import oddt\n",
    "from oddt.fingerprints import PLEC\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import matthews_corrcoef, precision_recall_curve, accuracy_score, auc\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import parallel_backend\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import glob\n",
    "import tempfile\n",
    "import hyperopt\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK, space_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb330b-d153-4ab3-b7c9-e09cc88dcdab",
   "metadata": {},
   "source": [
    "## 2. Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f82a79-564d-464c-bb0f-d913db69b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(y, pred_y, pred_y_prob):\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y, pred_y)\n",
    "    precision = precision_score(y, pred_y)\n",
    "    recall = recall_score(y, pred_y)\n",
    "    f1 = f1_score(y, pred_y)\n",
    "    roc_auc = roc_auc_score(y, pred_y_prob)\n",
    "    avg_precision = average_precision_score(y, pred_y_prob)\n",
    "    logloss = log_loss(y, pred_y_prob)\n",
    "    mcc = matthews_corrcoef(y, pred_y)\n",
    "    kappa = cohen_kappa_score(y, pred_y)\n",
    "    \n",
    "    # Create a DataFrame to store the evaluation metrics\n",
    "    evaluation_df = pd.DataFrame({\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\", \"Avg Precision\", \"Log Loss\", \"MCC\", \"Kappa\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1, roc_auc, avg_precision, logloss, mcc, kappa]\n",
    "    })\n",
    "    \n",
    "    return evaluation_df\n",
    "\n",
    "# Example usage:\n",
    "# y_true = actual_labels\n",
    "# pred_y = predicted_labels\n",
    "# pred_y_prob = predicted_probabilities\n",
    "# evaluation_df = evaluate_performance(y_true, pred_y, pred_y_prob)\n",
    "# print(evaluation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823b920-fb3e-402e-80d9-72c86cc531ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def enrichment_factor(y_true, y_score, percentage=1, pos_label=None, kind='fold'):\n",
    "    \"\"\"Computes enrichment factor for given percentage, i.e. EF_1% is\n",
    "    enrichment factor for first percent of given samples. This function assumes\n",
    "    that results are already sorted and samples with best predictions are first.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape=[n_samples]\n",
    "        True binary labels, in range {0,1} or {-1,1}. If positive label is\n",
    "        different than 1, it must be explicitly defined.\n",
    "\n",
    "    y_score : array, shape=[n_samples]\n",
    "        Scores for tested series of samples\n",
    "\n",
    "    percentage : int or float\n",
    "        The percentage for which EF is being calculated\n",
    "\n",
    "    pos_label: int\n",
    "        Positive label of samples (if other than 1)\n",
    "\n",
    "    kind: 'fold' or 'percentage' (default='fold')\n",
    "        Two kinds of enrichment factor: fold and percentage.\n",
    "        Fold shows the increase over random distribution (1 is random, the\n",
    "        higher EF the better enrichment). Percentage returns the fraction of\n",
    "        positive labels within the top x% of dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ef : float\n",
    "        Enrichment Factor for given percenage in range 0:1\n",
    "    \"\"\"\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    labels = y_true == pos_label\n",
    "    assert labels.sum() > 0, \"There are no correct predicions. Double-check the pos_label\"\n",
    "    assert len(labels) > 0, \"Sample size must be greater than 0\"\n",
    "    # calculate fraction of positve labels\n",
    "    n_perc = int(ceil(percentage / 100. * len(labels)))\n",
    "    out = labels[:n_perc].sum() / n_perc\n",
    "    if kind == 'fold':\n",
    "        out /= (labels.sum() / len(labels))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315c20b-516b-4c8b-b7ac-2534cbba4aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichment_factor_your_choice(data):\n",
    "        data_active_score = pd.DataFrame(data['test_score_C1'])\n",
    "        data_active_score['activity'] = test_plec_Cr['PLEC_4093']\n",
    "        data_active_score.sort_values('test_score_C1', inplace=True, ascending=False)\n",
    "        enrichment_value= round(enrichment_factor(data_active_score['activity'], data_active_score['test_score_C1'], percentage=1))\n",
    "        return enrichment_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa28639-942c-4c4a-9c0b-ffaf53cd2347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichment_factor_your_choice(data):\n",
    "    ef_values = []\n",
    "    for i in range(11):\n",
    "        data_active_score = pd.DataFrame(data.iloc[:, i])\n",
    "        data_active_score['activity'] = data['Observed_Activity']\n",
    "        data_active_score.sort_values(data.columns[i], inplace=True, ascending=False)\n",
    "        enrichment_value= round(enrichment_factor(data_active_score['activity'], data_active_score.iloc[:,0], percentage=1))\n",
    "        ef_values.append(enrichment_value)\n",
    "    return ef_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea884fd-6563-46b9-a1de-52ec11527e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_array_to_status(array):\n",
    "    # Define the mapping from 0 and 1 to \"inactive\" and \"active\"\n",
    "    status_mapping = {0: \"inactive\", 1: \"active\"}\n",
    "    \n",
    "    # Use list comprehension to map each element in the array\n",
    "    status_list = [status_mapping[element] for element in array]\n",
    "    \n",
    "    return status_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e45ef-9a25-4659-9603-c47883988497",
   "metadata": {},
   "source": [
    "## 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68ee36-e0b0-4e2b-8675-198fd4462604",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_test_set = pd.read_csv('path_to_TS1.csv_file',index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6955d5-aeb0-465b-990f-4915428236df",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_test_set['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255290b-8630-4fbc-b0c4-0e278fa0b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_example_test_set, y_example_test_set = example_test_set.drop(['int_class', 'class'], axis= 1), example_test_set['int_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5092bcaa-ba36-4b2d-a195-03f4e13e44b3",
   "metadata": {},
   "source": [
    "## 4. Load Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa82b0b-aa9f-498d-925c-2e555240edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/juni/working/ythdf/models/ANN_PLEC_MCTD8.sav'\n",
    "ANN_PLEC = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9e4e4-9e2e-47ac-9ce4-8b83c93a704e",
   "metadata": {},
   "source": [
    "## 5. predict class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b989e7-0515-4ba5-b3c3-418e3062c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_prediction=ANN_PLEC.predict(X_example_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4075b6-c20a-454e-98e2-530b68a19fb0",
   "metadata": {},
   "source": [
    "## 6. Map active and inactive to 0,1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f2c4f-f81b-4420-a5cc-4faee753fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_list = map_array_to_status(activity_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb51ab3-4e1c-40c4-a9d3-3e7d28ad3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_test_set['predicted_class'] = status_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a2576-6a55-4430-99fe-a757731c6937",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1a438-efce-4e5c-ae8f-b2622f4e9e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
